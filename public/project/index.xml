<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | R. Calen Walshe</title>
    <link>/calenwalshe.github.io/project/</link>
      <atom:link href="/calenwalshe.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 24 Sep 2019 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/calenwalshe.github.io/img/icon-192.png</url>
      <title>Projects</title>
      <link>/calenwalshe.github.io/project/</link>
    </image>
    
    <item>
      <title>Calculation of error rates for Bayesian decision models.</title>
      <link>/calenwalshe.github.io/project/classify/</link>
      <pubDate>Tue, 24 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/calenwalshe.github.io/project/classify/</guid>
      <description>&lt;p&gt;The classification accuracy between two general Gaussian distributions, and their discriminibality $d&amp;rsquo;$, are used ubiquitously to express the performance in binary classification and detection tasks. However, these quantities cannot in general be evaluated analytically from the Gaussian parameters. Standard numerical methods may require integration grids that are inefficiently large and fine, may converge slowly, yet miss relevant regions unless tailored case-by-case. We present a new calculation method, based on a transformation of the feature space, that is reliable and fast, and requires no hand-tailoring, for all cases up to 3 dimensions. &lt;a href=&#34;https://github.com/abhranildas/classify&#34; target=&#34;_blank&#34;&gt;Our open-source MATLAB implementation of this method&lt;/a&gt; provides a suite of tools related to such classification.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Predicting gaze with reinforcement learning</title>
      <link>/calenwalshe.github.io/project/reinforcement-learning/</link>
      <pubDate>Sat, 21 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/calenwalshe.github.io/project/reinforcement-learning/</guid>
      <description>&lt;p&gt;We developed an approach to predicting eye movements during category learning tasks. One of the central roles of eye movements is to sample information from the environment in a way that supports ongoing tasks and such that the information is sampled at the appropriate time.&lt;/p&gt;

&lt;p&gt;During classification tasks information from different spatial locations must often be used to determine the specific features of an object that are relevant for placing it in the appropriate category. When categories and objects are well known to the object they will typically know where to look on the object to classify it appropriately. However, when encountering a new object, or a new &lt;em&gt;type&lt;/em&gt; of object, it isn&amp;rsquo;t clear which features are relevant. In this case, the observer must learn the features that should be used to classify the object correctly.&lt;/p&gt;

&lt;p&gt;In this work, we show that a reinforcement learning approach to learning which locations to look at in order to perform well in the classification task does a good job of predicting actual human learning in classification experiments. This work supports the idea that reinforcement learning provides a good model for how humans select actions.&lt;/p&gt;

&lt;p&gt;Barnes, J. I., McColeman, C., Stepanova, E., Blair, M. R., &amp;amp; Walshe, R. C. (2014). RLAttn: An actor-critic model of eye movements during category learning. In Proceedings of the Annual Meeting of the Cognitive Science Society (Vol. 36, No. 36).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Predicting human gaze and action with deep learning</title>
      <link>/calenwalshe.github.io/project/deep-learning/</link>
      <pubDate>Sat, 21 Sep 2019 00:00:00 +0000</pubDate>
      <guid>/calenwalshe.github.io/project/deep-learning/</guid>
      <description>&lt;p&gt;Large-scale public datasets have been shown to benefit research in multiple areas of modern artificial intelligence. For decision-making research that requires human data, high-quality datasets serve as important benchmarks to facilitate the development of new methods by providing a common reproducible standard. Many human decision-making tasks require visual attention to obtain high levels of performance. Therefore, measuring eye movements can provide a rich source of information about the strategies that humans use to solve decision-making tasks. Here, we provide a large-scale, high-quality dataset of human actions with simultaneously recorded eye movements while humans play Atari video games. The dataset consists of 117 hours of gameplay data from a diverse set of 20 games, with 8 million action demonstrations and 328 million gaze samples. We introduce a novel form of gameplay, in which the human plays in a semi-frame-by-frame manner. This leads to near-optimal game decisions and game scores that are comparable or better than known human records. We demonstrate the usefulness of the dataset through two simple applications: predicting human gaze and imitating human demonstrated actions. The quality of the data leads to promising results in both tasks. Moreover, using a learned human gaze model to inform imitation learning leads to an 115% increase in game performance. We interpret these results as highlighting the importance of incorporating human visual attention in models of decision making and demonstrating the value of the current dataset to the research community. We hope that the scale and quality of this dataset can provide more opportunities to researchers in the areas of visual attention, imitation learning, and reinforcement learning.&lt;/p&gt;

&lt;p&gt;Zhang, R., Walshe, R.C., Liu, Z., Guan, L., Muller, K.S., Whritner, J.A., Zhang, L., Hayhoe, M. &amp;amp; Ballard, D. (2019). Atari-HEAD: Atari Human Eye-Tracking and Demonstration Dataset. preprint arXiv. arXiv:1903.06754 (Under review at AAAI’20).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A model of the retina</title>
      <link>/calenwalshe.github.io/project/retina/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/calenwalshe.github.io/project/retina/</guid>
      <description>&lt;p&gt;Many of the projects that I work on involve understanding the earliest stage of visual processing, the retina. There is a critical factor the take into account when building models of visual sensory processing which is the substantial loss in resolution that occurs in the periphery. In other words, when images on the retina fall at positions away from the center of the retina they are transmitted to the brain in a lower quality format.  This has major implications for perception and behavior. For example, doing basic signal detection is going to be negatively impacted when the images are presented in the periphery. Without taking the retinal factors into account predicting how humans detect signals is not likely to be successful. Retinal factors also play a major role in how well  we can search and located objects in scenes. Intuitively, finding objects that are very close to where you are currently looking is relatively easy compared to objects that are located far away. Part of this phenomenon is simply that the image of the object you are looking from the perspective of your brain is blurry and low-quality!&lt;/p&gt;

&lt;p&gt;The model of the retina that we developed recently has helped us ask concrete questions about basic signal processing capabilities of humans and machines. You can download a demo &lt;a href=&#34;https://github.com/calenwalshe/retina_model&#34; target=&#34;_blank&#34;&gt;here&lt;/a&gt; and filter any image you like at a specific location on the retina. See what the image looks like to your visual system!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian classification of objects in natural scenes.</title>
      <link>/calenwalshe.github.io/project/detection/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/calenwalshe.github.io/project/detection/</guid>
      <description>&lt;p&gt;Nearly all biological visual systems have the ability to separate relevant signals from background clutter. The natural-signals hypothesis suggests that biological systems exploit regularities in the statistical structure of natural scenes to solve this problem. Here, we study optimal detection of target signals that occlude part of the natural backgrounds they are presented on. Occluding targets are the most common in natural vision, but most of the existing literature has focused on additive targets because they are easier to work with both mathematically and experimentally. We develop a Bayes optimal detector for occluding targets that is limited by only the approximate sampling density of the primate retina and the natural scene statistics after retinal sampling. The performance of the optimal model is then compared to data measured in a human psychophysical detection task. To represent the scene statistics used by the Bayes detector we first filter natural scene patches with and without the target by a modulation transfer function that approximates the optics of the human eye. Second, we simulate the output of RGCs by blurring and downsampling the optically filtered image to match the expected retinal response at a given eccentricity. Then, we decompose the information relevant for detection of occluding targets. Luminance, pattern and boundary signals are computed separately for each patch. The variances and covariances of the features are then measured for a large set of background conditions and retinal eccentricities. Finally, performance of the optimal detector is measured in a set of background and eccentricity conditions for which we have measured human psychophysical responses. Our results show that human performance is approximately proportional to optimal. We conclude that much of the variation in detecting occluding targets across the visual field arises from the statistical structure of natural scenes and the limitations of retinal sampling.&lt;/p&gt;

&lt;p&gt;Walshe, R. C., Sebastian, S., &amp;amp; Geisler, W. (2018). Ideal observer for detection of occluding targets in natural scenes in the fovea and periphery. Journal of Vision, 18(10), 629-629.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian optimal search</title>
      <link>/calenwalshe.github.io/project/search/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/calenwalshe.github.io/project/search/</guid>
      <description>&lt;p&gt;Searching the environment in a fast and efficient manner is a critical
capability for humans and many other animals. Normally, multiple fixations
are used to identify and localize targets. However, in the special case of
covert search the target must be identified and localized within a single
fixation. Here we present a theory of covert search that takes into account
the statistical variation in background images, the falloff in resolution and
sampling with retinal eccentricity, the increase in intrinsic location uncertainty with retinal eccentricity, and the prior probability of target presence
and target location in the image. The computational steps of the theory are
as follows.First, the effective prior probability distribution on target location
is computed from the prior and the intrinsic location uncertainty. Second,
the effective amplitude of the target (also dependent on retinal eccentricity) is computed and the target (if present) is added to the background.
Third, template responses are computed at each image location by taking
the dot product of a template (having the shape of target) with the image
and then adding a random sample of internal noise. Fourth, the responses
are correctly normalized by the sum of the internal noise variance and the
estimated variance due to external factors (the background statistics).Fifth,
the normalized responses are summed with the log of the effective prior on
target location to obtain values proportional to the posterior probability.If the
maximum of these values exceeds a criterion, the response is that the target
is present at the location of the maximum. The theory predicts that i) misses
occur more often than false alarms, ii) misses occur further in the periphery
than false alarms, and iii) these asymmetries decrease with increasing target
amplitude. Preliminary results show that the theoretical and human spatial
distribution of errors are similar.&lt;/p&gt;

&lt;p&gt;Walshe, R.C. &amp;amp; Geisler, W.S. (2019). Theory of Covert Search in Noise Backgrounds Correctly Predicts Asymmetrical Spatial Distributions of Misses and False Alarms. Annual meeting of the Vision Sciences Society.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Detection of camouflaged textures</title>
      <link>/calenwalshe.github.io/project/texture/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/calenwalshe.github.io/project/texture/</guid>
      <description>&lt;p&gt;Occurrences of camouflage in nature evoke fascination and wonder in us. Less appreciated are the forces that shaped their evolution: the visual systems of their predators and prey. Indeed, having been filtered by them, camouflage specimens—no matter how ingenuous—are poised just at the edge of detectability, their inventiveness
only testifying to the sophistication of detection machinery that pruned even slightly less crafty variants.&lt;/p&gt;

&lt;p&gt;Using theory, computation and experiment, we turn our inquiry to the visual resources and mechanisms that are harnessed for detecting camouflage in nature. In particular, we consider the scenario where the camouflaging animal has exactly mimicked its background texture. Any visual information usable for detection then lies only at its boundary. We begin therefore by defining the boundary mismatch: a computational measure of visual discontinuity at the boundary that putatively summarizes most of this available information. We then synthesize artificial stimuli using 1/f noise as the camouflage texture (this shares the same spatial frequency properties as natural images, but lacks further structure), and assess human performance on them with a
series of target-detection experiments.&lt;/p&gt;

&lt;p&gt;We find regular variation in the detectability of these stimuli as a function of their boundary mismatch, allowing us to measure boundary-mismatch thresholds against variations in task-relevant stimulus dimensions like luminance, contrast and duration. We shall extend this analysis to variations in the size, distance and shape of the target, and with naturalistic texture stimuli (see Portilla and Simoncelli, 2000). These ideas can also be brought to the question of engineering effective camouflage. The boundary mismatch measure allows us to computationally prescribe the best location on a background to hide against, and compare the effectiveness of different textures towards this goal. These computational results can be connected to actual detectability in such scenarios using the results of our psychophysical experiments.&lt;/p&gt;

&lt;p&gt;Das, A., Walshe, R.C. and Geisler, W.S. (2018) Understanding camouflage detection. Presented at Computational and Systems Neuroscience (COSYNE) Annual Meeting.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Saccadic Decision Making</title>
      <link>/calenwalshe.github.io/project/saccades/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>/calenwalshe.github.io/project/saccades/</guid>
      <description>&lt;p&gt;Saccadic eye movements are the primary vehicle by which human gaze is brought in alignment with vital visual information present in naturalistic scenes. Although numerous studies using the double-step paradigm have demonstrated that saccade preparation is subject to modification under certain conditions, this has yet to be studied directly within a naturalistic scene-viewing context. To reveal characteristic properties of saccade programming during naturalistic scene viewing, we contrasted behavior across three conditions. In the Static condition of the main experiment, double-step targets were presented following a period of stable fixation on a central cross. In a Scene condition, targets were presented while participants actively explored a naturalistic scene. During a Noise condition, targets were presented during active exploration of a 1/f noise-filtered scene. In Experiment 2, we measure saccadic responses in three Static conditions (Uniform, Scene, and Noise) in which the backgrounds are the same as Experiment 1 but scene exploration is no longer permitted. We find that the mechanisms underlying saccade modification generalize to both dynamic conditions. However, we show that a property of saccade programming known as the saccadic dead time (SDT), the interval prior to saccade onset during which a saccade may not be canceled or modified, is lower in the Static task than it is in the dynamic tasks. We also find a trend toward longer SDT in the Scene as compared with Noise conditions. We discuss the implication of these results for computational models of scene viewing, reading, and visual search tasks.&lt;/p&gt;

&lt;p&gt;Walshe, R.C. &amp;amp; Nuthmann, A. (2015). Mechanisms of saccadic decision making while encoding naturalistic scenes. Journal of Vision, 15(21).&lt;/p&gt;

&lt;p&gt;Walshe, R.C. &amp;amp; Nuthmann, A. (2014). Asymmetric control of fixation durations in scene viewing. Vision Research, 100, 38-46.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
