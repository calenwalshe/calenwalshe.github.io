[{"authors":["admin"],"categories":null,"content":"My work as a scientist has taken me in many directions that all have a common theme: to understand how the brain converts visual perception into intelligent action. The primary set of methods that I use in my research combines behavioral experiments and machine learning. The goal of the work is to develop algorithms that solve perceptual tasks such as object recognition and visual search in similar ways to humans.\n","date":1564444800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1564444800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/calenwalshe.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/calenwalshe.github.io/authors/admin/","section":"authors","summary":"My work as a scientist has taken me in many directions that all have a common theme: to understand how the brain converts visual perception into intelligent action. The primary set of methods that I use in my research combines behavioral experiments and machine learning. The goal of the work is to develop algorithms that solve perceptual tasks such as object recognition and visual search in similar ways to humans.","tags":null,"title":"R. Calen Walshe","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/calenwalshe.github.io/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/calenwalshe.github.io/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/calenwalshe.github.io/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/calenwalshe.github.io/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/calenwalshe.github.io/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/calenwalshe.github.io/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"d9add6d488a7c84197515dda6dd8dfff","permalink":"/calenwalshe.github.io/talk/cps_brown/","publishdate":"2019-09-20T00:00:00Z","relpermalink":"/calenwalshe.github.io/talk/cps_brown/","section":"talk","summary":"Presenting recent work on detection, imitation learning and search.","tags":[],"title":"Research progress report","type":"talk"},{"authors":null,"categories":null,"content":"The classification accuracy between two general Gaussian distributions, and their discriminibality $d\u0026rsquo;$, are used ubiquitously to express the performance in binary classification and detection tasks. However, these quantities cannot in general be evaluated analytically from the Gaussian parameters. Standard numerical methods may require integration grids that are inefficiently large and fine, may converge slowly, yet miss relevant regions unless tailored case-by-case. We present a new calculation method, based on a transformation of the feature space, that is reliable and fast, and requires no hand-tailoring, for all cases up to 3 dimensions. Our open-source MATLAB implementation of this method provides a suite of tools related to such classification.\n","date":1569283200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569283200,"objectID":"42e651ee748fc9ff067d5300932233fb","permalink":"/calenwalshe.github.io/project/classify/","publishdate":"2019-09-24T00:00:00Z","relpermalink":"/calenwalshe.github.io/project/classify/","section":"project","summary":"The classification accuracy between two general Gaussian distributions, and their discriminibality $d\u0026rsquo;$, are used ubiquitously to express the performance in binary classification and detection tasks. However, these quantities cannot in general be evaluated analytically from the Gaussian parameters. Standard numerical methods may require integration grids that are inefficiently large and fine, may converge slowly, yet miss relevant regions unless tailored case-by-case. We present a new calculation method, based on a transformation of the feature space, that is reliable and fast, and requires no hand-tailoring, for all cases up to 3 dimensions.","tags":["machine_learning"],"title":"A new method to calculate classification accuracy","type":"project"},{"authors":null,"categories":null,"content":"Large-scale public datasets have been shown to benefit research in multiple areas of modern artificial intelligence. For decision-making research that requires human data, high-quality datasets serve as important benchmarks to facilitate the development of new methods by providing a common reproducible standard. Many human decision-making tasks require visual attention to obtain high levels of performance. Therefore, measuring eye movements can provide a rich source of information about the strategies that humans use to solve decision-making tasks. Here, we provide a large-scale, high-quality dataset of human actions with simultaneously recorded eye movements while humans play Atari video games. The dataset consists of 117 hours of gameplay data from a diverse set of 20 games, with 8 million action demonstrations and 328 million gaze samples. We introduce a novel form of gameplay, in which the human plays in a semi-frame-by-frame manner. This leads to near-optimal game decisions and game scores that are comparable or better than known human records. We demonstrate the usefulness of the dataset through two simple applications: predicting human gaze and imitating human demonstrated actions. The quality of the data leads to promising results in both tasks. Moreover, using a learned human gaze model to inform imitation learning leads to an 115% increase in game performance. We interpret these results as highlighting the importance of incorporating human visual attention in models of decision making and demonstrating the value of the current dataset to the research community. We hope that the scale and quality of this dataset can provide more opportunities to researchers in the areas of visual attention, imitation learning, and reinforcement learning.\nZhang, R., Walshe, R.C., Liu, Z., Guan, L., Muller, K.S., Whritner, J.A., Zhang, L., Hayhoe, M. \u0026amp; Ballard, D. (2019). Atari-HEAD: Atari Human Eye-Tracking and Demonstration Dataset. preprint arXiv. arXiv:1903.06754 (Under review at AAAIâ€™20).\n","date":1569024000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569024000,"objectID":"d561dc20b1562e523c4a63c9c55b0264","permalink":"/calenwalshe.github.io/project/deep-learning/","publishdate":"2019-09-21T00:00:00Z","relpermalink":"/calenwalshe.github.io/project/deep-learning/","section":"project","summary":"Incorporating human gaze improves imitation learning of human actions in ATARI games.","tags":["machine_learning"],"title":"Using deep learning to predict human gaze and human action.","type":"project"},{"authors":["R. Calen Walshe"],"categories":null,"content":"How much evidence do you need to be convinced in something that is very implausible is actually the case? Or rather, if you really don\u0026rsquo;t believe something to be true, how much evidence do you need to push you all the way to near certainty?\nOf course, this is a question about probabilities so we aren\u0026rsquo;t talking about any kind of knowledge that can be gained without evidence. The kind, for example, where we make definitions and then reason according to a set of rules to get some new knowledge. This kind of reasoning is common in mathematics or law. Most scientific laws, and especially in the rough and tumble world of knowledge in the real world (is this burger fully cooked, is Jones going to arrive late for the meeting again etc\u0026hellip;) we are always working with probability and hence using some data to assign a definite number to a possible state of the world.\nLet\u0026rsquo;s think about an interesting case. You believe that something is very implausible but not all the way down at 0. For example, someone claims to have the miraculous ability to communicate with the deceased. For me, this is very unlikely but I\u0026rsquo;m willing to entertain at least the possibility that it could be true. How low is the probability? I think somewhere around -75 dB. How did I get that number? Basically, I asked myself how many yes/no questions would they have to get right about someone I knew who was deceased that only I would know the answer to. I found by introspecting that after 25 correct responses I would notice a change in my belief. Of course, this number depends on the conditions of the test. This would be 25 questions that I was pretty sure they simply could not know. That\u0026rsquo;s pretty low, much lower than most Hollywood films where the clairvoyant hooks people in with a couple guesses but that\u0026rsquo;s where I\u0026rsquo;m at.\nBut how many more guesses would be need to get to really make a big dent in my belief about their powers of clairvoyance? The answer is that with a prior that low more evidence will certainly result in a change in my beliefs but it will never approach certainty. There are always some alternative hypothesis that I would entertain that could explain the data I\u0026rsquo;m observing. These alternatives might also be extremely low, say down at around -50 dB but the important point is that there are competitor hypotheses that are at least more plausible to me than clairvoyance.\nThis kind of thinking comes from a straightforward application of Bayes\u0026rsquo; rule to combine evidence and prior beliefs to determine the probability of a hypothesis.\nFor the clairvoyance case: C is the proposition that the person has clairvoyance and D is the proposition that they got some number of guesses correct.\nAccording to Bayes rule:\n$$Pr(C \\mid D) = Pr(D \\mid C) * Pr\u0026copy; / Pr(D)$$\nI found my prior is $Pr(D) = -75~\\text{dB}$.\nThe part of this that makes it very difficult to get to near certainty in a case like this is that the value of the denominator $Pr(D)$ is formed by conditioning on a possibly large number of alternative hypothesis, in addition to the hypothesis that the person has clairvoyance: the person happened to actually know the person we are referring to, I\u0026rsquo;m a contestant on a twisted game show, there is a hidden microphone and they are getting answers from someone outside the room etc\u0026hellip; These are all implausible, but less implausible than clairvoyance. To get the probability of the denominator we have by the law of total probability:\n$$Pr(D) = Pr(D \\mid C) Pr(\\text{C}) + \\sum_{i=1}^{N} Pr(D \\mid A_i) Pr(A_i)$$\nIf those alternative hypotheses stay more plausible than clairvoyance we are going to stay pretty close to the initially low prior belief.\nAlthough this example is contrived it has relevance to the real world, especially if one believes that human reasoning is approximately rational. An important footnote here is that nowhere in this setup are we dealing with true probabilities. It\u0026rsquo;s a statement about subjective states of affairs and how beliefs ought to change under those conditions.\n","date":1564444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564444800,"objectID":"aaef528886492fd5b32aa34aa0739d15","permalink":"/calenwalshe.github.io/post/unbelieve/","publishdate":"2019-07-30T00:00:00Z","relpermalink":"/calenwalshe.github.io/post/unbelieve/","section":"post","summary":"How much evidence do you need to be convinced in something that is very implausible is actually the case? Or rather, if you really don\u0026rsquo;t believe something to be true, how much evidence do you need to push you all the way to near certainty?\nOf course, this is a question about probabilities so we aren\u0026rsquo;t talking about any kind of knowledge that can be gained without evidence. The kind, for example, where we make definitions and then reason according to a set of rules to get some new knowledge.","tags":["Academic"],"title":"Is there something so unbelievable that you would assign it probability 0?","type":"post"},{"authors":[],"categories":null,"content":"","date":1558742400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558742400,"objectID":"e451f22907fa14d23f1fb772cf95bc8f","permalink":"/calenwalshe.github.io/talk/vss2019_abh/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/calenwalshe.github.io/talk/vss2019_abh/","section":"talk","summary":"","tags":[],"title":"A new method to calculate classification accuracy.","type":"talk"},{"authors":[],"categories":null,"content":"","date":1558742400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558742400,"objectID":"f491574fbb85fc16c17d5d88ad389590","permalink":"/calenwalshe.github.io/talk/vss2019/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/calenwalshe.github.io/talk/vss2019/","section":"talk","summary":"Spoiler alert - Human do pretty well!","tags":[],"title":"Comparing Bayes optimal search performance with human search.","type":"talk"},{"authors":["Zhang, R.","et al."],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"55a645f15b0d76adfa4d69c4f4f20462","permalink":"/calenwalshe.github.io/publication/deep/","publishdate":"2019-09-19T00:00:00Z","relpermalink":"/calenwalshe.github.io/publication/deep/","section":"publication","summary":"Large-scale public datasets have been shown to benefit research in multiple areas of modern artificial intelligence. For decision-making research that requires human data, high-quality datasets serve as important benchmarks to facilitate the development of new methods by providing a common reproducible standard. Many human decision-making tasks require visual attention to obtain high levels of performance. Therefore, measuring eye movements can provide a rich source of information about the strategies that humans use to solve decision-making tasks. Here, we provide a large-scale, high-quality dataset of human actions with simultaneously recorded eye movements while humans play Atari video games. The dataset consists of 117 hours of gameplay data from a diverse set of 20 games, with 8 million action demonstrations and 328 million gaze samples. We introduce a novel form of gameplay, in which the human plays in a semi-frame-by-frame manner. This leads to near-optimal game decisions and game scores that are comparable or better than known human records. We demonstrate the usefulness of the dataset through two simple applications, predicting human gaze and imitating human demonstrated actions. The quality of the data leads to promising results in both tasks. Moreover, using a learned human gaze model to inform imitation learning leads to an 115\\% increase in game performance. We interpret these results as highlighting the importance of incorporating human visual attention in models of decision making and demonstrating the value of the current dataset to the research community. We hope that the scale and quality of this dataset can provide more opportunities to researchers in the areas of visual attention, imitation learning, and reinforcement learning.","tags":["Source Themes"],"title":"Atari-HEAD: Atari Human Eye-Tracking and Demonstration Dataset","type":"publication"},{"authors":["R. Calen Walshe"],"categories":null,"content":"I probably should have seen this prior to now but there is a lot to read and even good stuff slips through the cracks. It was suggested to a colleague and he presented the work during a journal club. I was impressed by the work.\nVisual salience has been a hot term in vision science since the 90s. The salience model identifies local regions in an image that have higher priority than other local regions. Sometimes it is used to mean areas of an image that will many fixations and sometimes it is used for properties of the image that attract attention without necessarily leading to an eye-movement.\nThe schematic shows the basic idea behind the salience model. These ideas were inspired by the understanding the authors had at the time of how visual information propagates through the primate visual system. These ideas are still influential today and even if researchers don\u0026rsquo;t work on the salience model itself there are a lot of interest in the components of the salience model as individual research topics. The idea that the neural code expresses maps of one kind or another is probably the best hope we have of decoding the computations in the brain at a sufficiently rich level.\nThe schematic also shows that there are a lot of moving parts in the salience model. It is a complicated model and this has lead to many arguements. In my opinion, many of the debates over the salience model have been needlessly fussy.\nThis brings me to what I like about Kienzle et al. First of all, they attempt to do what the salience model does: predict eye-movements. While the salience model makes assumptions about image features the authors of this paper make minimal assumptions by taking a data-driven approach.\nThey collect many eye-movements in a generic task. By comparing regions that are fixated with regions not fixated they are able to learn a receptive field (filter) that characterizes image salience about as well as the salience model.\nThe summary figure for the machine learnt salient shows how their setup works. They learn the kernels from the eye-movement data. Their model learned four kernels. Local image regions are filtered by the kernels, then passed through a point non-linearity and summed up. The resulting value is the salience. There are some additional analyses showing performs comparably to the salience model on several metrics.\nThis paper is not new, and since this there have been more applications of machine learning to salience. Their approach is simple and successful. As a technical advance there is clear progress. Like many approaches to studying biological vision with machine learning there is a tradeoff between opacity and predictability. Some models are hard to understand but predict well, while others perform conversely. The Kienzle et al. method is not as black box as some others but the connection to the underlying biology is less clear than in the salience model.\n","date":1554163200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554163200,"objectID":"35d9fdb32e70b9ef229a699e1a9aba90","permalink":"/calenwalshe.github.io/post/visionscience/","publishdate":"2019-04-02T00:00:00Z","relpermalink":"/calenwalshe.github.io/post/visionscience/","section":"post","summary":"I probably should have seen this prior to now but there is a lot to read and even good stuff slips through the cracks. It was suggested to a colleague and he presented the work during a journal club. I was impressed by the work.\nVisual salience has been a hot term in vision science since the 90s. The salience model identifies local regions in an image that have higher priority than other local regions.","tags":["Academic"],"title":"\"Bringing machine learning to vision science before it was hot\"","type":"post"},{"authors":["R. Calen Walshe"],"categories":null,"content":"I think a lot about detection and estimation problems. A detection problem is one in which it pays off to identify the presence or absence of a signal amongst some background noise. This is a problem that has applications all over the place. Transmission of data over the internet relies on detection of discrete pulses encoded in an analog signal. Doctors \u0026quot;detect\u0026quot; cancerous masses in images taken with an x-ray or other technology. Estimation is a different but related problem. In an estimation problem the challenge is to figure out which state the world is in when we only receive some noisy corrupted sample from the world. It is like detection but with estimation we want more than \u0026quot;yes\u0026quot; or \u0026quot;no\u0026quot; we want to know the value behind the noisy samples.\nI think about detection and estimation because they are problems that visual systems are confronted with all the time. They are both problems that biology has solved over and over again to allow organisms to use sense data (signals encoded by the sense organs like the eyes and ears) to understand the environment they are in.\nThey are also well posed computation problems. When the setup is simple enough there are ways to characterize exactly how biological systems \u0026quot;should\u0026quot; do detection and estimation. In these cases we don't have to settle just for \u0026quot;how\u0026quot; detection and estimation comes about.\nThe concept of normalization has been an important principle in sensory neuroscience for a number of decades. There are several distinct ideas that the term can refer to but generally speaking it refers to a variety of computations that take the sensory inputs and divide (normalize) them by some other quantity. There are at least two good reasons why normalization occurs in these circuits. One reason is that it provides a mechanism for response gain. Neurons have physical limits on their minimum and maximum activity. Normalization helps expand the range over which neurons take inputs and improves sensitivity of the response.\nAnother interesting and subtle role for normalization is that it provides improved statistical power for systems, like neural networks, that are responsible for detection and estimation and that have large variations in how reliable the units of the system (neurons) are. Improving the statistical power yeilds higher signal to noise, better estimation (accuracy) and lower error rates (detection). All of those things are complimentary ways of saying that normalization is necessary to perform well under conditions of large and portentially varying uncertainty.\nThere are some good ways to argue for the inferential benefits of normalization, but not many are simpler than simulation.\nThe figures below are the result of a toy simulation estimating the mean from noisy samples with and without normalization. The trick here is that while 100 samples are drawn, each of those 100 samples is drawn from a distribution with a different variance but the same mean. The differing variance means that each of the 100 samples differs in the amount of information that they give about the population mean. However, they all contribute something and they also should be combined to come up with an estimate that uses all available information. In this example, normalization is used to weight each sample by an estimate of its reliability. With normalization the population mean is estimated as a weighted mean of the samples where the weights are the reliability. Without normalization we simply compute the unweighted mean. This results in all samples contributing to the estimated mean in an equal manner. Keeping with the analogy to neurons, even if a neuron is very noisy (has responses that are high or low to the same stimulus) and is known to be noisy the rest of the brain would still listen to that neuron just as closely as a neuron that was very reliable and consistent. With the described normalization setup this noisy neuron would be downweighted and its contribution minimized.\nThe left panel has the results of the no normalization experiment and the right panel has the normalized results. The different coloured lines show what happens when we try and combine different numbers of samples. When averaging things we generally expected the estimate to get better with more samples, normalization or not. The x axis is the way I set the range of noisy units (neurons) in this experiment. The range of variances in the sample range from 0 to an upper limit. The x axis shows that upper limit. A value of 20 here means that the variances of the sample population are from 1 to 20. The values on the y axis is the measure of error. Larger values mean worse estimation performance.\n\nTo reiterate the analogy to neurons: a single sample is a hypothetical response of a single neuron; all of the samples form a population of neurons. The samples are combined either in a normalized or unnormalized way; the same problem would be true for the population of neurons as well. Neurons differ in how noisy they are, so any rule that combines them would need to take the variation in reliability into account.\nThe results of the toy simulation show in a straightforward way how the biological rule of cortical normalization can lead to computational benefits for detection and estimation.\nlibrary(purrr) true.mu \u0026lt;- 20 sigma.var \u0026lt;- seq(1, 20, length.out = 5) n.samples \u0026lt;- seq(10, 100, 10) b.norm \u0026lt;- c(0,1) param.frame \u0026lt;- expand.grid(true.mu = true.mu, sigma.var = sigma.var, n.samples = n.samples, b.norm = b.norm) param.frame.data \u0026lt;- by_row(param.frame, function(x) { n.rms \u0026lt;- 1000 noise.fixed \u0026lt;- seq(1,x$sigma.var,length.out = x$n.samples) obs.sample \u0026lt;- replicate(n.rms, rnorm(x$n.samples, x$true.mu, noise.fixed)) if(x$b.norm) { sigma_hat \u0026lt;- apply(obs.sample, 1, var) norm.val \u0026lt;- (1/sigma_hat) / sum(1/sigma_hat) obs.sample.1 \u0026lt;- sweep(obs.sample, 1, norm.val, \u0026quot;*\u0026quot;) obs.sample.2 \u0026lt;- colSums(obs.sample.1) error \u0026lt;- mean((obs.sample.2 - true.mu)^2) }else{ obs.sample.1 \u0026lt;- sweep(obs.sample, 1, 1/x$n.samples, \u0026quot;*\u0026quot;) obs.sample.2 \u0026lt;- colSums(obs.sample.1) error \u0026lt;- mean((obs.sample.2 - true.mu)^2) } data.frame(error = error) }, .to = \u0026quot;simulate_col\u0026quot;) param.frame.data.unnest \u0026lt;- param.frame.data %\u0026gt;% unnest()  ","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"d724a2192589b67bd32276549becde3c","permalink":"/calenwalshe.github.io/post/normalization/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/calenwalshe.github.io/post/normalization/","section":"post","summary":"I think a lot about detection and estimation problems. A detection problem is one in which it pays off to identify the presence or absence of a signal amongst some background noise. This is a problem that has applications all over the place. Transmission of data over the internet relies on detection of discrete pulses encoded in an analog signal. Doctors \u0026quot;detect\u0026quot; cancerous masses in images taken with an x-ray or other technology.","tags":["Academic"],"title":"Nature is pretty good at building machines.","type":"post"},{"authors":["R. Calen Walshe"],"categories":null,"content":"Patterns that reduce uncertainty about some aspect of the world are called information. Open your curtains in the morning, see a certain amount of light. Opening your window gives you some information about what time of the morning it is. More light is a pretty reliable way of approximating the time during sunrise.\nSpeaking generally, it always seems right that more information is a good thing. If getting more information reduces our uncertainty and we want to be more certain, then collect as much information as possible. This feels right. In fact, it feels so right that it is the default operating mode for most people. Reading news daily or watching news hourly is a pretty common pasttime. The goal here must be to reduce the uncertainty we have about the world. When the news isn't out with a straighforward mission to decieve us we'll be getting some good nuggets of information that we can store away for later use in our quest to understand the world. Sure, some of those nuggets will be worse than others but we believe that information accumulates so we'll take what we can get.\nIs more information always a good thing? In some circumstances it surely is. Here is a common scenario. There is a fixed pool of information sources that vary in their quality. If we sample them all and then combine them in a rational way then the total information will always be greater with more sources. So that is +1 for the more information camp. The cable news addicts.\nChange the setup a little bit. I'm going to change it in a way that is a little more realistic (not much) for how we actually use information.\nLet's say that there is fixed pool of information bearing samples. The samples vary in their quality, some are low information some are high. Then let's say that we can't sample all the sources -- we can only sample a fixed subset of those samples. Think of this like having a large collection of memories about the world but only a couple of them can be recalled in a given instant while we're thinking about something. Furthermore, for simplicity, assume that which samples form the subset are random. This is also kind of consistent with how memory works. There is obviously some structure to which events appear in memory, but there is also a arbitrary random quality to them as well. Now, once this subset of samples has been encoded we combine them in a rational way. This is a case where more information is not always better.\nIf the low quality samples increase in proportion to the high quality samples then the probability of selecting a high quality information sample goes down. The result is that the overall information is lower. If all you have access to is good information then total information will be good.\nWe can compare these two situations exactly using a simple simulation. The code is at the bottom of the page.\nFor situation 1 set n.good equal to total.samples. Set p.bad to 0. This means there are only good samples. Information increases.\nInformation is not clearly defined here. It is not in \u0026quot;bits\u0026quot; or \u0026quot;nats\u0026quot; -- units that result from a typical definition of information. The units here are related to how well a binary stimulus can be classified. Larger numbers mean we have more information to classify with. These units could be converted into Shannon information and we would get the same result.\nFor situation 2 set n.good to some number and set total.samples to some value lower than that. Then increase p.bad and see what happens. What this does is increase the proportion of low information samples relative to high information samples. The figures show what happens.\nIn the following, the proportion of bad samples increases and information goes down. \nNext, the total number of samples increase and all are sampled, and the information goes up. \ng \u0026lt;- function(n.good, p.bad, total.samples) { n.good \u0026lt;- n.good n.bad \u0026lt;- p.bad * n.good mu.snr.bad \u0026lt;- -log2(.99) mu.snr.good \u0026lt;- 1/sqrt(10) snr.vector \u0026lt;- c(rep(mu.snr.bad, times = round(n.bad)), rep(mu.snr.good, round(n.good))) f \u0026lt;- function(n.samples) { info.sample \u0026lt;- sample(snr.vector, n.samples, replace = F) } info.samples \u0026lt;- replicate(1000, f(total.samples)) mean(sqrt(colSums(info.samples^2))) } out.vals \u0026lt;- lapply(seq(0, 1, .01), FUN = function(x) g(100, x, 10)) snr.frame \u0026lt;- data.frame(x = seq(0, 1, .01), y = unlist(out.vals))  ","date":1553558400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553558400,"objectID":"5cefd43dacf0726c1040765d418f5e19","permalink":"/calenwalshe.github.io/post/information/","publishdate":"2019-03-26T00:00:00Z","relpermalink":"/calenwalshe.github.io/post/information/","section":"post","summary":"Patterns that reduce uncertainty about some aspect of the world are called information. Open your curtains in the morning, see a certain amount of light. Opening your window gives you some information about what time of the morning it is. More light is a pretty reliable way of approximating the time during sunrise.\nSpeaking generally, it always seems right that more information is a good thing. If getting more information reduces our uncertainty and we want to be more certain, then collect as much information as possible.","tags":["Academic"],"title":"On news addiction and reading good things not more things.","type":"post"},{"authors":["R. Calen Walshe"],"categories":null,"content":"The Monty Hall puzzle is entertaining because the result turns out to be counter to intuition. Those are the most entertaining puzzles. The family of puzzles like Monty Hall are so contrived that they don't resemble reality. Reality, especially when chance and probability is involved is not expected to be more intuitive than these puzzles. I like them because they are fun to think about but they also remind me how easy it for intuition to lead to wrong conclusions when not applied under the right circumstances.\nHere is another puzzle that resembles the Monty Hall in this regard.\nYou are entered in a chess tournament. To win the big prize you have to win two matches in a row out of three matches that you will played. You will have two opponents, Magnus Carlsen and the local chess pro. Of course, the probability of beating Carlsen is lower than beating the pro. What you get to choose is the order you play them in. You can either choose to play Magnus, the pro, then Magnus or play the pro, Magnus, then the pro again. Which sequence would you choose?\nThe obvious choice is to select the sequence in which you play the pro twice and Magnus once, since Magnus is the better player. But we only suspect that is right, how do we justify our choice? Here is a simple argument by reasoning via probability:\nLet the probability of beating Magnus be \\(Pr(Carlsen) = c\\) and the probability of beating the pro, \\(Pr(Pro) = p\\) and let \\(c .\nThe union of independent events is \\(Pr(\\bigcup\\limits_{i=1}^N A_i) = \\sum\\limits_{i = 1}^N Pr(A_i)\\).\nIf we enumerate all the independent ways to win the contest under the first sequence it looks like this:\n\\[Pr(CPC) = cpc + cp(1 - c) + (1-c)pc\\]\nThe first case is winning all three matches, the second is winning the first two and losing the second and the third is losing the first match and winning the second and third. \\(1 - c\\) expresses the probability of losing to Magnus.\nLikewise, the enumeration of the second sequence is as follows:\n\\[Pr(PCP) = pcp + pc(1 - p) + (1 - p)cp\\]\nRearrange the terms in these sums and we get \\(cp(2 - c)\\) for the first sequence and \\(cp(2 - p)\\) for the second.\nNow it's easy to see that since \\(c then \\(cp(2 - c)  cp(2 - p)\\). Which is surprising because it is not as I would have thought prior running through this argument. Even though Magnus is a better player, it is still worthwhile playing him twice under these conditions. With this sequence the worse player is in the middle of the sequence and that gives a higher probability of getting two wins in a row.\n","date":1552176000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552176000,"objectID":"f91b8a4b72644dcf219f5adab3c8598f","permalink":"/calenwalshe.github.io/post/probability_puzzle/","publishdate":"2019-03-10T00:00:00Z","relpermalink":"/calenwalshe.github.io/post/probability_puzzle/","section":"post","summary":"The Monty Hall puzzle is entertaining because the result turns out to be counter to intuition. Those are the most entertaining puzzles. The family of puzzles like Monty Hall are so contrived that they don't resemble reality. Reality, especially when chance and probability is involved is not expected to be more intuitive than these puzzles. I like them because they are fun to think about but they also remind me how easy it for intuition to lead to wrong conclusions when not applied under the right circumstances.","tags":["Academic"],"title":"When playing Magnus Carlsen, play him twice.","type":"post"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/calenwalshe.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/calenwalshe.github.io/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":[],"categories":null,"content":"","date":1540425600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1540425600,"objectID":"6c51f4e7db6f6671209b011c94c9da3f","permalink":"/calenwalshe.github.io/talk/faculty/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/calenwalshe.github.io/talk/faculty/","section":"talk","summary":"","tags":[],"title":"Presenting work on detection, natural signals and scene statistics.","type":"talk"},{"authors":["R. Calen Walshe"],"categories":null,"content":"The Monty Hall problem is a classic. I like it because it posed with simple language and hides it\u0026rsquo;s complexity. The first time you hear the puzzle you have a good chance of answering correctly (50\u0026frasl;50). However, once you start to think about the reason for your answer you might need to grab a beer and get ready to think a little bit. As the story goes, one of the 20th centuries most famous mathematicians, Paul Erdos, needed several days before he was convinced by the truth of the proposed solution.\nFor a full description of the game check out the wikipedia page. The version I\u0026rsquo;m going to describe is stripped down.\nThere are three doors. Behind one of the doors there is a car and behind the other two doors there are goats. You player is invited to make a guess as to which door hides the car. After making a selection the host will open one of the doors that has a goat. Now, the player is given the opportunity to switch their choice to another door or to stick with their current selection. What would you do?\nIf you are like Paul Erdos you would initially think that it doesn\u0026rsquo;t make any difference whether you switch or not. The probability of selecting the car on the first choice is 1\u0026frasl;3. Swapping doesn\u0026rsquo;t change this basic fact. This is the incorrect intuition that many people have for the problem. The correct solution is that one should always swap.\nThe best way to show this is with conditional probabilities. In other words, you show that the probability that the car is behind door two given you initially selected door one and the host removed door three is higher than 1\u0026frasl;3. This is done for us on the wikipedia page.\nMy own intuition for the problem goes as follows:\nThe probability of getting the car on the first try is 1\u0026frasl;3 and the probability of not getting it is 2\u0026frasl;3.\nNow, break down the switching into these two cases:\nIf you happened to guess right the first time switching will be wrong. This happens 1\u0026frasl;3 of the time.\nOn the other hand, if you happened to guess wrong on the first try then switching will always get you the correct door. This happens 2\u0026frasl;3 of the time.\nIt\u0026rsquo;s really as simple as this. Adopting a strategy of switching will result in getting the car 2\u0026frasl;3 of the time. A big improvement over the 1\u0026frasl;3 that we get from staying.\nAnother way to demonstrate this without going to conditional probabilities is run a simulation and see what probabilities come out. Run this code in R.\nnum_trials \u0026lt;- 100000 # number of trials car \u0026lt;- sample(1:3, num_trials,replace = T) # set the door with the car choice \u0026lt;- sample(1:3, num_trials, replace = T) # set the door choice of the contestant remove_options \u0026lt;- apply(cbind(car,choice), 1, function (x) setdiff(c(1,2,3), x)) # the door(s) the host can choose to remove remove \u0026lt;- lapply(remove_options, function(x) ifelse(length(x) \u0026gt; 1, sample(x, 1), x)) # randomly select a door to remove swap\u0026lt;- unlist(apply(cbind(choice, remove), 1, function(x) setdiff(c(1,2,3), x))) # always swap door sum(swap == car)/length(car) # proportion of swapped choices that match the car door  ","date":1467849600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1467849600,"objectID":"dd2504258fa1a8907b70e20ef964d3fe","permalink":"/calenwalshe.github.io/post/montyhall/","publishdate":"2016-07-07T00:00:00Z","relpermalink":"/calenwalshe.github.io/post/montyhall/","section":"post","summary":"The Monty Hall problem is a classic. I like it because it posed with simple language and hides it\u0026rsquo;s complexity. The first time you hear the puzzle you have a good chance of answering correctly (50\u0026frasl;50). However, once you start to think about the reason for your answer you might need to grab a beer and get ready to think a little bit. As the story goes, one of the 20th centuries most famous mathematicians, Paul Erdos, needed several days before he was convinced by the truth of the proposed solution.","tags":["Academic"],"title":"Did Monty consider that someone might want the goat?","type":"post"},{"authors":null,"categories":null,"content":"Many of the projects that I work on involve understanding the earliest stage of visual processing, the retina. There is a critical factor the take into account when building models of visual sensory processing which is the substantial loss in resolution that occurs in the periphery. In other words, when images on the retina fall at positions away from the center of the retina they are transmitted to the brain in a lower quality format. This has major implications for perception and behavior. For example, doing basic signal detection is going to be negatively impacted when the images are presented in the periphery. Without taking the retinal factors into account predicting how humans detect signals is not likely to be successful. Retinal factors also play a major role in how well we can search and located objects in scenes. Intuitively, finding objects that are very close to where you are currently looking is relatively easy compared to objects that are located far away. Part of this phenomenon is simply that the image of the object you are looking from the perspective of your brain is blurry and low-quality!\nThe model of the retina that we developed recently has helped us ask concrete questions about basic signal processing capabilities of humans and machines. You can download a demo here and filter any image you like at a specific location on the retina. See what the image looks like to your visual system!\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"21654c7568f9317922b4d44d0e8bda8f","permalink":"/calenwalshe.github.io/project/retina/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/calenwalshe.github.io/project/retina/","section":"project","summary":"We built a model of the retina and then use it to estimate fundamental limits of human visual capabilities.","tags":["signal_detection"],"title":"A model of the retina","type":"project"},{"authors":null,"categories":null,"content":"Nearly all biological visual systems have the ability to separate relevant signals from background clutter. The natural-signals hypothesis suggests that biological systems exploit regularities in the statistical structure of natural scenes to solve this problem. Here, we study optimal detection of target signals that occlude part of the natural backgrounds they are presented on. Occluding targets are the most common in natural vision, but most of the existing literature has focused on additive targets because they are easier to work with both mathematically and experimentally. We develop a Bayes optimal detector for occluding targets that is limited by only the approximate sampling density of the primate retina and the natural scene statistics after retinal sampling. The performance of the optimal model is then compared to data measured in a human psychophysical detection task. To represent the scene statistics used by the Bayes detector we first filter natural scene patches with and without the target by a modulation transfer function that approximates the optics of the human eye. Second, we simulate the output of RGCs by blurring and downsampling the optically filtered image to match the expected retinal response at a given eccentricity. Then, we decompose the information relevant for detection of occluding targets. Luminance, pattern and boundary signals are computed separately for each patch. The variances and covariances of the features are then measured for a large set of background conditions and retinal eccentricities. Finally, performance of the optimal detector is measured in a set of background and eccentricity conditions for which we have measured human psychophysical responses. Our results show that human performance is approximately proportional to optimal. We conclude that much of the variation in detecting occluding targets across the visual field arises from the statistical structure of natural scenes and the limitations of retinal sampling.\nWalshe, R. C., Sebastian, S., \u0026amp; Geisler, W. (2018). Ideal observer for detection of occluding targets in natural scenes in the fovea and periphery. Journal of Vision, 18(10), 629-629. (Published abstract).\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"477813931710146f41d6177009f4258a","permalink":"/calenwalshe.github.io/project/detection/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/calenwalshe.github.io/project/detection/","section":"project","summary":"We build a statistically near-optimum detector and compare this with human behavior.","tags":["signal_detection"],"title":"Detecting objects in natural scenes.","type":"project"},{"authors":null,"categories":null,"content":"Occurrences of camouflage in nature evoke fascination and wonder in us. Less appreciated are the forces that shaped their evolution: the visual systems of their predators and prey. Indeed, having been filtered by them, camouflage specimensâ€”no matter how ingenuousâ€”are poised just at the edge of detectability, their inventiveness only testifying to the sophistication of detection machinery that pruned even slightly less crafty variants.\nUsing theory, computation and experiment, we turn our inquiry to the visual resources and mechanisms that are harnessed for detecting camouflage in nature. In particular, we consider the scenario where the camouflaging animal has exactly mimicked its background texture. Any visual information usable for detection then lies only at its boundary. We begin therefore by defining the boundary mismatch: a computational measure of visual discontinuity at the boundary that putatively summarizes most of this available information. We then synthesize artificial stimuli using 1/f noise as the camouflage texture (this shares the same spatial frequency properties as natural images, but lacks further structure), and assess human performance on them with a series of target-detection experiments.\nWe find regular variation in the detectability of these stimuli as a function of their boundary mismatch, allowing us to measure boundary-mismatch thresholds against variations in task-relevant stimulus dimensions like luminance, contrast and duration. We shall extend this analysis to variations in the size, distance and shape of the target, and with naturalistic texture stimuli (see Portilla and Simoncelli, 2000). These ideas can also be brought to the question of engineering effective camouflage. The boundary mismatch measure allows us to computationally prescribe the best location on a background to hide against, and compare the effectiveness of different textures towards this goal. These computational results can be connected to actual detectability in such scenarios using the results of our psychophysical experiments.\nDas, A., Walshe, R.C. and Geisler, W.S. (2018) Understanding camouflage detection. Presented at Computational and Systems Neuroscience (COSYNE) Annual Meeting.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8cd9ff6bfed682089bf66769c8523c54","permalink":"/calenwalshe.github.io/project/texture/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/calenwalshe.github.io/project/texture/","section":"project","summary":"How studied how edges are used by human vision to detect camouflage.","tags":["signal_detection"],"title":"Detection of camouflaged textures","type":"project"},{"authors":null,"categories":null,"content":"Searching the environment in a fast and efficient manner is a critical capability for humans and many other animals. Normally, multiple fixations are used to identify and localize targets. However, in the special case of covert search the target must be identified and localized within a single fixation. Here we present a theory of covert search that takes into account the statistical variation in background images, the falloff in resolution and sampling with retinal eccentricity, the increase in intrinsic location uncertainty with retinal eccentricity, and the prior probability of target presence and target location in the image. The computational steps of the theory are as follows.First, the effective prior probability distribution on target location is computed from the prior and the intrinsic location uncertainty. Second, the effective amplitude of the target (also dependent on retinal eccentricity) is computed and the target (if present) is added to the background. Third, template responses are computed at each image location by taking the dot product of a template (having the shape of target) with the image and then adding a random sample of internal noise. Fourth, the responses are correctly normalized by the sum of the internal noise variance and the estimated variance due to external factors (the background statistics).Fifth, the normalized responses are summed with the log of the effective prior on target location to obtain values proportional to the posterior probability.If the maximum of these values exceeds a criterion, the response is that the target is present at the location of the maximum. The theory predicts that i) misses occur more often than false alarms, ii) misses occur further in the periphery than false alarms, and iii) these asymmetries decrease with increasing target amplitude. Preliminary results show that the theoretical and human spatial distribution of errors are similar.\nWalshe, R.C. \u0026amp; Geisler, W.S. (2019). Theory of Covert Search in Noise Backgrounds Correctly Predicts Asymmetrical Spatial Distributions of Misses and False Alarms Annual meeting of the Vision Sciences Society (Poster).\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e1c328db12859ff36e126fc391510388","permalink":"/calenwalshe.github.io/project/search/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/calenwalshe.github.io/project/search/","section":"project","summary":"We developed an algorithm to optimally search in natural scenes. We tested humans against our optimal benchmark.","tags":["eye_movements"],"title":"Optimal Search in Scenes","type":"project"},{"authors":null,"categories":null,"content":"Saccadic eye movements are the primary vehicle by which human gaze is brought in alignment with vital visual information present in naturalistic scenes. Although numerous studies using the double-step paradigm have demonstrated that saccade preparation is subject to modification under certain conditions, this has yet to be studied directly within a naturalistic scene-viewing context. To reveal characteristic properties of saccade programming during naturalistic scene viewing, we contrasted behavior across three conditions. In the Static condition of the main experiment, double-step targets were presented following a period of stable fixation on a central cross. In a Scene condition, targets were presented while participants actively explored a naturalistic scene. During a Noise condition, targets were presented during active exploration of a 1/f noise-filtered scene. In Experiment 2, we measure saccadic responses in three Static conditions (Uniform, Scene, and Noise) in which the backgrounds are the same as Experiment 1 but scene exploration is no longer permitted. We find that the mechanisms underlying saccade modification generalize to both dynamic conditions. However, we show that a property of saccade programming known as the saccadic dead time (SDT), the interval prior to saccade onset during which a saccade may not be canceled or modified, is lower in the Static task than it is in the dynamic tasks. We also find a trend toward longer SDT in the Scene as compared with Noise conditions. We discuss the implication of these results for computational models of scene viewing, reading, and visual search tasks.\nWalshe, R.C. \u0026amp; Nuthmann, A. (2015). Mechanisms of saccadic decision making while encoding naturalistic scenes. Journal of Vision, 15(21).\nWalshe, R.C. \u0026amp; Nuthmann, A. (2014). Asymmetric control of fixation durations in scene viewing. Vision Research, 100, 38-46.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"1e374a88a9dbaedc984bd5aaa0d0e771","permalink":"/calenwalshe.github.io/project/saccades/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/calenwalshe.github.io/project/saccades/","section":"project","summary":"How does the visual system decide when to move the eyes when viewing natural scenes?","tags":["eye_movements"],"title":"Saccadic Decision Making","type":"project"},{"authors":["R. Calen Walshe"],"categories":null,"content":"I had a coffee on campus with a friend in the Engineering department. To him, it was completely obvious that the way that perception should work is that your perceptions should recover exactly what is contained in the distribution of light that strikes the eye. It was an interesting discussion and the following sums up some of our discussion.\nThe story of colour vision begins in a part of the eye known as the retina. Within the retina there are hundreds of millions of photosensitive cells called photoreceptors. The primary role of these cells is to convert light (electromagnetic radiation) into electrochemical signals that are passed to regions of the brain that are responsible for vision. In primates, the most important early visual area is called Striate cortex and is located at the back of the brain.\nThe type of photoreceptor that takes care of signaling colour is called a cone and primates have three different types of them. Each of the cone classes is distinguished by the wavelength of electromagnetic radiation that they are most sensitive to. L-cones have the strongest response to long wavelength, M-cones respond to medium wavelengths and S-cones respond to short wavelengths.\nWhen light strikes the retina the responses of these three different cone classes are combined. The relative responses of these three different types of cones forms that basis of the brains ability to estimate the distribution of the incoming light and perceive it as a particular colour.\nIn other words, the cones response function only allows them to respond to light with specific wavelengths. Electromagnetic radiation that falls too far outside the range of wavelengths that a cone can respond to will not influence a cones response and therefore will not influence colour perception.\nThere is also a slightly deeper issue to consider here. The goal of colour vision isn\u0026rsquo;t actually about estimating the spectral power distribution (distribution of wavelengths) of a light source. More correctly, it\u0026rsquo;s about using spectral information to track regularities that exist out there in the world.\nTwo perceptual phenomena help to make this clear, colour constancy and colour metamers.\nColour constancy is the label that is used to describe an aspect of perception in which an object is perceived to have a relatively uniform colour despite large variations in the wavelengths being emitted from the object. The following illusion is a stark demonstration of this principle. In the illusion, the patches labelled A and B are actually the exact same colour even though the perception of the patches is very different. This is colour constancy at work. The brain is doing some work to estimate the colour that would be expected based on the surrounding context and is discounting the colour that is actually being emitted from the patch.\nColour metamers also show that the colour of light and the emitted wavelength are not one and the same. A colour metamer occurs when the perceived colour is identical but the spectral power distribution is completely different. The existence of metamers comes about because the retina is compressing a continuous distribution of wavelengths into responses of three different cone classes.\nIn summary, the perception of colour at a location in the world is influenced by more than the wavelength of light that arises from it. The context of the perception matters.\n","date":1458777600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1458777600,"objectID":"904b5e803679bb41b33d27a99247bb0e","permalink":"/calenwalshe.github.io/post/colour/","publishdate":"2016-03-24T00:00:00Z","relpermalink":"/calenwalshe.github.io/post/colour/","section":"post","summary":"I had a coffee on campus with a friend in the Engineering department. To him, it was completely obvious that the way that perception should work is that your perceptions should recover exactly what is contained in the distribution of light that strikes the eye. It was an interesting discussion and the following sums up some of our discussion.\nThe story of colour vision begins in a part of the eye known as the retina.","tags":["Academic"],"title":"Where in the world is red?","type":"post"},{"authors":["R. Calen Walshe"],"categories":null,"content":"The Titanic left on it\u0026rsquo;s maiden voyage on April 10, 1912 from Southampton to New York City. It is one of the most recognizable and iconic disasters of the 20th century. Of the 2,224 passengers and crew that set sail about 1500 of them perished when the ship hit an iceberg in the North Atlantic. The catastrophe led to Engineers rethinking the idea that a combination of hull size and durable steel would result in an unsinkable mass. The Titanic remains a symbol of the perils of engineering hubris.\nA notable aspect that has received much attention is the lack of lifeboats present on the ship. The 20 lifeboats that were present were capable of taking only 1,178 passengers whereas the Titanic had a maximum capacity of 3,327. Clearly, these numbers don\u0026rsquo;t add up. Either the engineers had been drinking and designing or they simply could not fathom the idea of the Titanic sinking. I suggest a mixture of both. The sad result of the lack of lifeboats is that the majority of the passengers were unable to secure a position on a raft and perished with the sinking of the ship.\nTaking a closer look at the casualties reveals characteristics of early 20th century culture. The first is a preference to prioritize the safety of women and children. Looking at the difference in survival between women and men, the risk of perishing was 27% for women, whereas men overall had a 81% risk of perishing. Nearly all children aboard the ship survived. Given the lack of lifeboats, the women and children first policy practiced within maritime code was an important factor in determining the increased rates of survivorship amongst women.\nThere is a more unpleasant aspect that arises from the survivor data from the Titanic. The data starkly demonstrate how wealth and class can influence exposure to risk, even under highly volatile disaster situations.\n   Class Gender Risk Odds     1 female 0.03 0.04   1 male 0.66 1.93   2 female 0.11 0.13   2 male 0.85 5.84   3 female 0.51 1.04   3 male 0.85 5.57    The odds of perishing in the sinking of the Titanic are 26 times higher if you are a women in third class than if you are a woman in first class and about 3 times higher for men in third class. This is a powerful indication of how the systems that were in place to protect passengers were biased towards protecting those with wealth. In other words, those who could afford a first class ticket were not only given greater luxury but their lives were given a higher priority as well.\nEvents like the Titanic can be highly revelatory. The Titanic was an important event in changing opinions about disaster preparedness. Large scale, unusual, and impactful events do not occur very often but when they do their novelty provides a unique window into the operation of systems that often remain obscured from view.\n","date":1456876800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1456876800,"objectID":"4754f5fed8c7f6040c1d9213683a7098","permalink":"/calenwalshe.github.io/post/titanic/","publishdate":"2016-03-02T00:00:00Z","relpermalink":"/calenwalshe.github.io/post/titanic/","section":"post","summary":"The Titanic left on it\u0026rsquo;s maiden voyage on April 10, 1912 from Southampton to New York City. It is one of the most recognizable and iconic disasters of the 20th century. Of the 2,224 passengers and crew that set sail about 1500 of them perished when the ship hit an iceberg in the North Atlantic. The catastrophe led to Engineers rethinking the idea that a combination of hull size and durable steel would result in an unsinkable mass.","tags":["Academic"],"title":"On the lives lost in the sinking of the Titanic.","type":"post"},{"authors":["R. Calen Walshe"],"categories":null,"content":"The Alamo Draft House is a local theatre company here in Austin. Their bread and butter is putting on the current hits but they also make a solid focus on showing cult hits. Over at the Ritz yesterday they put on the Princess Bride (1987) a love story staring a young Claire Underwood (Robin Wright) in the innocent days before being corrupted by Frank and the dirty lies of the US Senate.\nOne of the great scenes in this film is when the masked hero, Wesley, challenges Vizzini, one of the villians, to a battle of wits. The scene is humourous, but it has some underlying lessons for students of game theory and the psychology of human decision making. The scene begins as Vizzini clutches the Princess Buttercup (aka Claire Underwood/Robin Wright) and theatens to kill her if the hero Wesley steps any closer. Vizzini has cause to be worried, Wesley has just won a duel with a master swordsman and rendered a Giant unconscious in hand to hand combat. Vizzini does not possess any physical strength, but he does proclaim himself to have a massive intellect. Here is what Vizzini has to say about himself:\nHero: You\u0026rsquo;re that smart?\nVizzini: Let me put it this way: have you ever heard of Plato, Aristotle, Socrates?\nHero: Yes.\nVizzini: Morons.\nSo Vizzini is smart. Therefore, the only way for our hero to overcome Vizzini is to challenge him to a game of wits and this is exactly what he does. Being Ã¼ber confident in his abundant intellect Vizzini accepts the challenge.\nHere is the game they play:\nThere are two cups of wine on the table. The masekd hero hides the two cups of wine and informs Vizzini that he has placed a powerful poison, Iocane, in one of the cups. Each contestant will drink from one of the cups with Vizzini getting to chose who drinks which cup. The stakes are that whoever drinks from the poised cup will presumably die.\nSo why is this game wits? At first blush, it might seem like this is more a game of pure chance. Sort of like playing Russian Roulette with an equal number of blanks and bullets. What makes this into a game of wits is when we consider the possibility that Vizzini could have a reasonably good model of the Hero\u0026rsquo;s psychological processes that went into choosing which cup to place the poison in. In other words, Vizzini could attempt to read the Hero\u0026rsquo;s mind.\nVizzini is, \u0026ldquo;like, a really smart person\u0026rdquo; and he verbalises his reasoning process:\nHero: All right: where is the poison? The battle of wits has begun. It ends when you decide and we both drink, and find out who is right and who is dead.\nVizzini: But it\u0026rsquo;s so simple. All I have to do is divine from what I know of you. Are you the sort of man who would put the poison into his own goblet, or his enemy\u0026rsquo;s? [pauses to study the Hero] Now, a clever man would put the poison into his own goblet, because he would know that only a great fool would reach for what he was given. I\u0026rsquo;m not a great fool, so I can clearly not choose the wine in front of you. But you must have known I was not a great fool; you would have counted on it, so I can clearly not choose the wine in front of me.\nHero: You\u0026rsquo;ve made your decision then?\nVizzini: Not remotely. Because iocaine comes from Australia, as everyone knows. And Australia is entirely peopled with criminals. And criminals are used to having people not trust them, as you are not trusted by me. So I can clearly not choose the wine in front of you.\nHero: Truly, you have a dizzying intellect.\nVizzini: Wait till I get going! Where was I?\nHero: Australia.\nVizzini: Yes \u0026ndash; Australia, and you must have suspected I would have known the powder\u0026rsquo;s origin, so I can clearly not choose the wine in front of me.\nHero: [beginning nervousness] You\u0026rsquo;re just stalling now.\nVizzini: You\u0026rsquo;d like to think that, wouldn\u0026rsquo;t you? You\u0026rsquo;ve beaten my giant, which means you\u0026rsquo;re exceptionally strong. So, you could have put the poison in your own goblet, trusting on your strength to save you. So I can clearly not choose the wine in front of you. But, you\u0026rsquo;ve also bested my Spaniard which means you must have studied. And in studying, you must have learned that man is mortal so you would have put the poison as far from yourself as possible, so I can clearly not choose the wine in front of me.\nHero: [nervously] You\u0026rsquo;re trying to trick me into giving away something \u0026ndash; it won\u0026rsquo;t work \u0026ndash;\nVizzini: [triumphant] It has worked \u0026ndash; you\u0026rsquo;ve given everything away \u0026ndash; I know where the poison is.\nHero: [fool\u0026rsquo;s courage] Then make your choice.\nVizzini: I will. And I choose [stops suddenly and points at something behind the Man in Black] what in the world can that be?\nHero: [Turns, looks] What? Where? I don\u0026rsquo;t see anything.\nVizzini quickly switches the goblets while the Hero has his head turned.\nVizzini: Oh, well, I-I could have sworn I saw something. No matter.\nThe Hero turns to face him again. Vizzini starts to laugh.\nHero: What\u0026rsquo;s so funny?\nVizzini: I\u0026rsquo;ll tell you in a minute. First, let\u0026rsquo;s drink \u0026ndash; me from my glass, and you from yours.\nAnd he picks up his goblet. The Hero picks up the one in front of him. As they both start to drink, Vizzini hesitates a moment. Allowing the MAN IN BLACK to drink first, he swallows his wine.\nHero: You guessed wrong.\nVizzini: [roaring with laughter] You only think I guessed wrong\u0026hellip; [louder now] \u0026hellip;that\u0026rsquo;s what\u0026rsquo;s so funny! I switched glasses when your back was turned. Ha-ha, you fool.\nThe Hero sits silently.\nVizzini: You fell victim to one of the classic blunders. The most famous is \u0026ldquo;Never get involved in a land war in Asia.\u0026rdquo; But only slightly less well known is this: \u0026ldquo;Never go in against a Sicilian when death is on the line!Ahahahaha, ahahahaha, ahahaha\u0026ndash;\nGame theory is one of the jewels of 20th century intellectual achievements. It gives us a way to understand games such as the one that is played here by provided a framework for understanding how certain games \u0026lsquo;ought\u0026rsquo; to be played when the players are making choices that are in their best interests. In game theory, you will often see the term rationality being bandied about. In many game theoretic settings the goal is determine what a rational decision maker would do given the available information and a certain goal. Rational typically means self interested because a rational decision is one that maximizes the payoff for individual and does not consider the harm to the competitor.\nSo let\u0026rsquo;s assume that Vizzini is a rational decision maker. In a two player game such as we have in the film the information that he gets from the arrangement of the cups is nil. Instead, Vizzini must try and determine whether he can exploit any hidden patterns in the Hero\u0026rsquo;s reasoning and exploit this pattern to gain information about which cup has the poison. In this case, Vizzini is assuming that the Hero is not a fully rational agent, if he was then there would be no way to get better than a 50\u0026frasl;50 chance at determining which cup has the poison.\nIn reality, it would probably be impossible for Vizzini to make use of any such reasoning to any effect. Firstly, he doesn\u0026rsquo;t know the Hero which means he doesn\u0026rsquo;t have much input to calculate the Hero\u0026rsquo;s decision making biases. Secondly, even if he did know the Hero very well it is likely to be well beyond the capability of human reasoning to determine the role of decision making biases in such a case as there are non-linearities in the way that multiple sources of information are combined to make a decision. The result is that any signal will be almostly completely obscured by the background noise.\nHowever, in simpler games that are actually quite similar to Wesley\u0026rsquo;s game we can actually obtain solutions. Consider the game of buying a car. The game is to get the best price you can on the car that you want. In other words, you want to beat the car sellers market. You might discover that during Winter there are the greatest number of cars put up on the market. Being smart, like Vizzini, you might think that this will lead you to get a deal. Not so fast, this is a bit too easy. Other buyers probably are able to figure this out as well and there might very well be a glut of buyers in the market and thus lead to inflated sales prices. So know you need to ask yourself, how far is the average car buyer likely to take this? If other car buyers know that other car buyers know that spring is a good time to buy a car maybe they\u0026rsquo;ll buy a car in the Fall instead, which is the period of the year during which the second highest volume of cars is available. Buyers might take this even one step further and realize that other car buyers might have access to the same information. This might bias them to pick another time of the year to buy. In other words, the real challenge here is figuring out how deep the other buyers estimates go and what information that have access to. If all buyers had access to the same information and were equally rational there would be no gaming the system. However, in human systems this is patently not the case and opportunities for advantage can emerge.\nThe New York Times has an interactive example of a variant of the Princess Bride game. See how you do!\n","date":1455926400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1455926400,"objectID":"ec03d7ba8ee33144de265269e802084c","permalink":"/calenwalshe.github.io/post/vizini/","publishdate":"2016-02-20T00:00:00Z","relpermalink":"/calenwalshe.github.io/post/vizini/","section":"post","summary":"The Alamo Draft House is a local theatre company here in Austin. Their bread and butter is putting on the current hits but they also make a solid focus on showing cult hits. Over at the Ritz yesterday they put on the Princess Bride (1987) a love story staring a young Claire Underwood (Robin Wright) in the innocent days before being corrupted by Frank and the dirty lies of the US Senate.","tags":["Academic"],"title":"The Princess Bride does game theory.","type":"post"},{"authors":["R. Calen Walshe"],"categories":null,"content":"A historical map distributed by the Rand McNally company. Time goes down the map and the relative size of the empire is given by the amount of area that it takes up at any specific time. What strikes me is the historical continuity presented in the map. For example, the Chinese empires rose and fell, but the civilization itself has remained active throughout all of recorded history.\nWe can also see the emergence of the Roman Empire in classical antiquity and its scale on the map shows the behemoth that it became. Rome was in conflict with all other major powers. Roman gains translated directly into losses for the other empires.\nI\u0026rsquo;m interested to know more about how the Persian empire transitioned into the Seleucid and Bactrian Kingdoms. Furthermore, what was the relationship between the Persians of ancient antiquity and the Sassanian Persians? The greatest asset of such a map is its ability to stimulate such questions.\n","date":1455667200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"a67d889026c0d694d756181af1ff4bbd","permalink":"/calenwalshe.github.io/post/history/","publishdate":"2016-02-17T00:00:00Z","relpermalink":"/calenwalshe.github.io/post/history/","section":"post","summary":"A historical map distributed by the Rand McNally company. Time goes down the map and the relative size of the empire is given by the amount of area that it takes up at any specific time. What strikes me is the historical continuity presented in the map. For example, the Chinese empires rose and fell, but the civilization itself has remained active throughout all of recorded history.\nWe can also see the emergence of the Roman Empire in classical antiquity and its scale on the map shows the behemoth that it became.","tags":["Academic"],"title":"History is beautiful.","type":"post"},{"authors":["R. Calen Walshe","Antje Nuthmann"],"categories":null,"content":"","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"c6c3c7f6797d61a186452ae648f2a565","permalink":"/calenwalshe.github.io/publication/jov/","publishdate":"2017-04-01T00:00:00Z","relpermalink":"/calenwalshe.github.io/publication/jov/","section":"publication","summary":"Saccadic eye movements are the primary vehicle by which human gaze is brought in alignment with vital visual information present in naturalistic scenes. Although numerous studies using the double-step paradigm have demonstrated that saccade preparation is subject to modification under certain conditions, this has yet to be studied directly within a naturalistic scene-viewing context. To reveal characteristic properties of saccade programming during naturalistic scene viewing, we contrasted behavior across three conditions. In the Static condition of the main experiment, double-step targets were presented following a period of stable fixation on a central cross. In a Scene condition, targets were presented while participants actively explored a naturalistic scene. During a Noise condition, targets were presented during active exploration of a 1/f noise-filtered scene. In Experiment 2, we measure saccadic responses in three Static conditions (Uniform, Scene, and Noise) in which the backgrounds are the same as Experiment 1 but scene exploration is no longer permitted. We find that the mechanisms underlying saccade modification generalize to both dynamic conditions. However, we show that a property of saccade programming known as the saccadic dead time (SDT), the interval prior to saccade onset during which a saccade may not be canceled or modified, is lower in the Static task than it is in the dynamic tasks. We also find a trend toward longer SDT in the Scene as compared with Noise conditions. We discuss the implication of these results for computational models of scene viewing, reading, and visual search tasks.","tags":["Source Themes"],"title":"Mechanisms of saccadic decision making while encoding naturalistic scenes","type":"publication"},{"authors":["R. Calen Walshe","Antje Nuthmann"],"categories":null,"content":"","date":1404172800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1404172800,"objectID":"a0b3deae14eb917959d0d9a413abb7dc","permalink":"/calenwalshe.github.io/publication/vr/","publishdate":"2017-04-01T00:00:00Z","relpermalink":"/calenwalshe.github.io/publication/vr/","section":"publication","summary":"In two experiments we investigated the control of fixation durations in naturalistic scene viewing. Empirical evidence from the scene onset delay paradigm and numerical simulations of such data with the CRISP model [Psychological Review 117 (2010) 382â€“405] have suggested that processing related difficulties may lead to prolonged fixation durations. Here, we ask whether processing related facilitation may lead to comparable decreases to fixation durations. Research in visual search and reading have reported only uni-directional shifts. To address the question of unidirectional (slow down) as opposed to bidirectional (slow down and speed up) adjustment of fixation durations in the context of scene viewing, we used a saccade-contingent display change method to either reduce or increase the luminance of the scene during prespecified critical fixations. Degrading the stimulus by shifting luminance down resulted in an immediate increase to fixation durations. However, clarifying the stimulus by shifting luminance upwards did not result in a comparable decrease to fixation durations. These results suggest that the control of fixation durations in scene viewing is asymmetric, as has been reported for visual search and reading.","tags":["Source Themes"],"title":"Asymmetrical control of fixation durations in scene viewing","type":"publication"},{"authors":["McColeman, C., Barnes, J., Chen, L., Meier, K., Walshe, R.C. \u0026 Blair, M."],"categories":null,"content":"","date":1391126400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1391126400,"objectID":"49e6103a81f4ff22b25408ee418d952b","permalink":"/calenwalshe.github.io/publication/plos/","publishdate":"2017-04-01T00:00:00Z","relpermalink":"/calenwalshe.github.io/publication/plos/","section":"publication","summary":"","tags":["Source Themes"],"title":"Learning-Induced Changes in Attentional Allocation during Categorization: A Sizable Catalog of Attention Change as Measured by Eye Movements","type":"publication"},{"authors":["R. Calen Walshe","Nuthmann, A."],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"c4daafc61e66cda899b3f0e3f3164d1e","permalink":"/calenwalshe.github.io/publication/cogsci/","publishdate":"2100-01-01T00:00:00Z","relpermalink":"/calenwalshe.github.io/publication/cogsci/","section":"publication","summary":"Several computational models explaining fixation durations in scene viewing (Nuthmann, Smith, Engbert, \u0026 Henderson, 2010) and in reading (Engbert, Nuthmann, Richter, \u0026 Kliegl, 2005; Reichle, Pollatsek, Fisher, \u0026 Rayner, 1998) assume that saccade programming is completed in two stages - an initial, labile stage that is subject to cancellation and an subsequent, non-labile stage in which the program can no longer be cancelled. This distinction is motivated by findings from doublestep experiments that used much simpler situations than scene viewing or reading. Here, we adopt a classic double-step paradigm to a scene-viewing context. In a Static condition targets are presented to the left or right of a central fixation cross along a horizontal axis while in a Scene condition targets are presented in a gaze contingent manner along a trajectory defined by the location of recent fixations. We found evidence in support of the claims that saccade cancellation occurs within a naturalistic scene-viewing context and that saccade cancellation can account for increases in observed fixation duration distributions. The duration of the non-labile stage was estimated to be longer in the Scene condition compared to the Static condition.","tags":["Source Themes"],"title":"Programming of saccades to double-step targets in scene viewing: A test of assumptions present in the CRISP model","type":"publication"},{"authors":["Blair, M., Watson, M., Walshe, R.C., Maj, F."],"categories":null,"content":"","date":1230768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1230768000,"objectID":"3fb05570c997ef8d48c4fd85998cd018","permalink":"/calenwalshe.github.io/publication/jep/","publishdate":"2017-04-01T00:00:00Z","relpermalink":"/calenwalshe.github.io/publication/jep/","section":"publication","summary":"Humans have an extremely flexible ability to categorize regularities in their environment, in part because of attentional systems that allow them to focus on important perceptual information. In formal theories of categorization, attention is typically modeled with weights that selectively bias the processing of stimulus features. These theories make differing predictions about the degree of flexibility with which attention can be deployed in response to stimulus properties. Results from 2 eye-tracking studies show that humans can rapidly learn to differently allocate attention to members of different categories. These results provide the first unequivocal demonstration of stimulus-responsive attention in a categorization task. Furthermore, the authors found clear temporal patterns in the shifting of attention within trials that follow from the informativeness of particular stimulus features. These data provide new insights into the attention processes involved in categorization.","tags":["Source Themes"],"title":"Extremely selective attention: Eye-tracking studies of the dynamic allocation of attention to stimulus features in categorization","type":"publication"},{"authors":["R. Calen Walshe"],"categories":null,"content":"A talk is planned at the Molatov Seminar series on Friday, October 12, 2018. The Molatov Seminars is described by its organizers as: \u0026ldquo;Molotov Seminar hosts talks at UT by anyone, for anyone, on anything.\u0026rdquo;\nI plan to do just that and will discuss economic inequality from a statistical perspective. A topic I don\u0026rsquo;t work on professionally but am very interested. I hope to stimulate a good discussion at the least.\nHere is a link to the announcement\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"5126bc5f9fb7d3157b13c6f8e554bf91","permalink":"/calenwalshe.github.io/post/public_talk/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/calenwalshe.github.io/post/public_talk/","section":"post","summary":"A talk is planned at the Molatov Seminar series on Friday, October 12, 2018. The Molatov Seminars is described by its organizers as: \u0026ldquo;Molotov Seminar hosts talks at UT by anyone, for anyone, on anything.\u0026rdquo;\nI plan to do just that and will discuss economic inequality from a statistical perspective. A topic I don\u0026rsquo;t work on professionally but am very interested. I hope to stimulate a good discussion at the least.","tags":["Academic"],"title":"Come out to a public talk I'll be giving on UT campus.","type":"post"}]